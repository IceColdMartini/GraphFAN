{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2deef35",
   "metadata": {},
   "source": [
    "# GFAN Seizure Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff852",
   "metadata": {},
   "source": [
    "Main execution notebook for the GFAN seizure detection pipeline. This notebook orchestrates data preprocessing, feature extraction, model training, and evaluation using Leave-One-Subject-Out cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f1e47",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0b8d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/Kazi/miniconda3/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: decorator in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.7 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (3.10.3)\n",
      "Requirement already satisfied: numpy<3,>=1.25 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (2.3.1)\n",
      "Requirement already satisfied: packaging in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.11 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (1.15.3)\n",
      "Requirement already satisfied: tqdm in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from pooch>=1.5->mne) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.6.15)\n",
      "Successfully installed mne\n",
      "Requirement already satisfied: pyedflib in /Users/Kazi/miniconda3/lib/python3.13/site-packages (0.1.42)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from pyedflib) (2.3.1)\n",
      "Successfully installed pyedflib\n",
      "Package installation completed!\n",
      "Requirement already satisfied: pyedflib in /Users/Kazi/miniconda3/lib/python3.13/site-packages (0.1.42)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/Kazi/miniconda3/lib/python3.13/site-packages (from pyedflib) (2.3.1)\n",
      "Successfully installed pyedflib\n",
      "Package installation completed!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = [\"mne\", \"pyedflib\"]\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b88e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Using regular tqdm instead of notebook version\n",
    "\n",
    "from src.data_preprocessing import CHBMITDataProcessor, load_chb_mit_annotations\n",
    "from src.spectral_decomposition import MultiScaleSTFT\n",
    "from src.graph_construction import create_graph_from_windows\n",
    "from src.training import EEGDataset, LeaveOneSubjectOutValidator\n",
    "from src.gfan_model import GFAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433c631",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'path': 'chb-mit-scalp-eeg-database-1.0.0',\n",
    "        'target_fs': 256,\n",
    "        'window_size': 2.0,\n",
    "        'overlap': 0.5,\n",
    "        'n_subjects_to_process': None  # Process ALL subjects with A6000's 48GB VRAM\n",
    "    },\n",
    "    'features': {\n",
    "        'window_sizes': [1.0, 2.0, 4.0],\n",
    "        'hop_ratio': 0.25,\n",
    "        'log_transform': True\n",
    "    },\n",
    "    'graph': {\n",
    "        'method': 'hybrid',\n",
    "        'spatial_weight': 0.5,\n",
    "        'functional_weight': 0.5\n",
    "    },\n",
    "    'model': {\n",
    "        'n_channels': 18, # Will be updated based on data\n",
    "        'spectral_features_dims': [129, 257, 513], # Will be updated\n",
    "        'hidden_dims': [256, 128, 64],  # Increased capacity for A6000\n",
    "        'n_classes': 2,\n",
    "        'sparsity_reg': 0.01,\n",
    "        'dropout_rate': 0.2,\n",
    "        'uncertainty_method': 'mc_dropout',\n",
    "        'fusion_method': 'attention'\n",
    "    },\n",
    "    'trainer': {\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'class_weights': [1.0, 10.0], # Will be updated based on data\n",
    "        'sparsity_weight': 0.01,\n",
    "        'kl_weight': 1e-6,\n",
    "        'epochs': 100,  # Increased for full dataset training\n",
    "        'batch_size': 128,  # Larger batch size for A6000\n",
    "        'gradient_accumulation_steps': 4  # Effective batch size: 512\n",
    "    },\n",
    "    'validation': {\n",
    "        'n_folds': None  # Full LOSO validation for complete evaluation\n",
    "    },\n",
    "    'results_path': 'results/final_run_summary.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5b3ff",
   "metadata": {},
   "source": [
    "### 3. Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal (MPS) for acceleration.\n"
     ]
    }
   ],
   "source": [
    "# Configure device for NVIDIA A6000 GPU environment\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    # Enable optimizations for A6000\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "config['trainer']['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f54713",
   "metadata": {},
   "source": [
    "### 4. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a91814",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CHBMITDataProcessor(\n",
    "    target_fs=config['data']['target_fs'],\n",
    "    window_size=config['data']['window_size'],\n",
    "    overlap=config['data']['overlap']\n",
    ")\n",
    "stft_extractor = MultiScaleSTFT(\n",
    "    fs=config['data']['target_fs'],\n",
    "    window_sizes=config['features']['window_sizes'],\n",
    "    hop_ratio=config['features']['hop_ratio']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8c1cc",
   "metadata": {},
   "source": [
    "### 5. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a98bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing for 3 subjects...\n",
      "\n",
      "Processing Subject 1/3: chb01\n",
      "  Loaded annotations for 42 files\n",
      "  Found 42 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 1:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb01_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 1\n",
      "\n",
      "Processing Subject 2/3: chb02\n",
      "  Loaded annotations for 36 files\n",
      "  Found 36 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 2:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb02_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 2\n",
      "\n",
      "Processing Subject 3/3: chb03\n",
      "  Loaded annotations for 38 files\n",
      "  Found 38 EDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_01.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files for subject 3:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 3598 windows from chb03_02.edf, seizure events: 0\n",
      "    Limited to 50 windows for memory safety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully processed 2/2 files for subject 3\n",
      "Final concatenation of all processed data...\n",
      "Windows shape: (300, 23, 512)\n",
      "Labels shape: (300,)\n",
      "Subjects: 300 total\n",
      "Scale 0 spectral features shape: (300, 23, 129, 9)\n",
      "Scale 1 spectral features shape: (300, 23, 257, 5)\n",
      "Scale 2 spectral features shape: (300, 23, 257, 5)\n",
      "\n",
      "Data processing summary:\n",
      "Total windows: 300\n",
      "Class distribution: {0.0: 300}\n",
      "Unique subjects: 3\n",
      "Spectral features shapes: [(300, 23, 129, 9), (300, 23, 257, 5), (300, 23, 257, 5)]\n",
      "Data processing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gc  # For garbage collection\n",
    "\n",
    "# Initialize storage\n",
    "all_windows, all_labels, all_subjects = [], [], []\n",
    "all_spectral_features = [[] for _ in config['features']['window_sizes']]\n",
    "\n",
    "subject_dirs = sorted([d for d in glob.glob(os.path.join(config['data']['path'], 'chb*')) if os.path.isdir(d)])\n",
    "# Process ALL subjects - no restrictions for A6000 environment\n",
    "print(f\"Processing ALL {len(subject_dirs)} subjects with full dataset...\")\n",
    "\n",
    "# Process each subject with optimized memory management for A6000\n",
    "for subject_idx, subject_dir in enumerate(subject_dirs):\n",
    "    print(f\"\\nProcessing Subject {subject_idx + 1}/{len(subject_dirs)}: {os.path.basename(subject_dir)}\")\n",
    "    \n",
    "    subject_id = int(os.path.basename(subject_dir).replace('chb', ''))\n",
    "    summary_file = os.path.join(subject_dir, f\"{os.path.basename(subject_dir)}-summary.txt\")\n",
    "    \n",
    "    try:\n",
    "        annotations = load_chb_mit_annotations(summary_file)\n",
    "        print(f\"  Loaded annotations for {len(annotations)} files\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load annotations for {subject_dir}. Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    edf_files = sorted(glob.glob(os.path.join(subject_dir, '*.edf')))\n",
    "    print(f\"  Found {len(edf_files)} EDF files\")\n",
    "    \n",
    "    # Process ALL files per subject - no memory restrictions\n",
    "    subject_windows = []\n",
    "    subject_labels = []\n",
    "    subject_spectral_features = [[] for _ in config['features']['window_sizes']]\n",
    "    \n",
    "    files_processed = 0\n",
    "    for edf_file in tqdm(edf_files, desc=f\"Processing files for subject {subject_id}\", leave=False):\n",
    "        file_name = os.path.basename(edf_file)\n",
    "        seizure_info = annotations.get(file_name, [])\n",
    "        \n",
    "        try:\n",
    "            windows, labels, channels = processor.process_file(edf_file, seizure_info)\n",
    "            if windows is None or len(windows) == 0:\n",
    "                print(f\"    No windows extracted from {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"    Extracted {len(windows)} windows from {file_name}, seizure events: {len(seizure_info)}\")\n",
    "            \n",
    "            # No window limits - process ALL windows with A6000's 48GB VRAM\n",
    "            print(f\"    Processing all {len(windows)} windows (no memory restrictions)\")\n",
    "            \n",
    "            # Extract spectral features in optimized batches for A6000\n",
    "            file_spectral_features = [[] for _ in config['features']['window_sizes']]\n",
    "            batch_size = 64  # Larger batch size for A6000's memory capacity\n",
    "            \n",
    "            for batch_start in range(0, len(windows), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(windows))\n",
    "                \n",
    "                for i in range(batch_start, batch_end):\n",
    "                    try:\n",
    "                        multiscale_stft = stft_extractor.compute_multiscale_stft(windows[i])\n",
    "                        for scale_idx, stft_result in enumerate(multiscale_stft):\n",
    "                            file_spectral_features[scale_idx].append(stft_result['magnitude'])\n",
    "                    except Exception as e:\n",
    "                        print(f\"      Warning: STFT failed for window {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Less frequent cleanup for better performance\n",
    "                if batch_start % (batch_size * 10) == 0:\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Add all processed data\n",
    "            if any(len(features) > 0 for features in file_spectral_features):\n",
    "                min_features = min(len(features) for features in file_spectral_features if len(features) > 0)\n",
    "                if min_features > 0:\n",
    "                    subject_windows.append(windows[:min_features])\n",
    "                    subject_labels.append(labels[:min_features])\n",
    "                    \n",
    "                    for scale_idx in range(len(config['features']['window_sizes'])):\n",
    "                        if len(file_spectral_features[scale_idx]) >= min_features:\n",
    "                            subject_spectral_features[scale_idx].extend(file_spectral_features[scale_idx][:min_features])\n",
    "                \n",
    "                files_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not process file {edf_file}. Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Add subject data to global collections\n",
    "    if len(subject_windows) > 0:\n",
    "        subject_windows_concat = np.concatenate(subject_windows, axis=0)\n",
    "        subject_labels_concat = np.concatenate(subject_labels, axis=0)\n",
    "        \n",
    "        all_windows.append(subject_windows_concat)\n",
    "        all_labels.append(subject_labels_concat)\n",
    "        all_subjects.extend([subject_id] * len(subject_windows_concat))\n",
    "        \n",
    "        for scale_idx in range(len(config['features']['window_sizes'])):\n",
    "            if len(subject_spectral_features[scale_idx]) > 0:\n",
    "                all_spectral_features[scale_idx].append(np.array(subject_spectral_features[scale_idx]))\n",
    "    \n",
    "    print(f\"  Successfully processed {files_processed}/{len(edf_files)} files for subject {subject_id}\")\n",
    "    print(f\"  Subject {subject_id} total windows: {len(subject_windows_concat) if len(subject_windows) > 0 else 0}\")\n",
    "    \n",
    "    # Periodic cleanup every 5 subjects\n",
    "    if (subject_idx + 1) % 5 == 0:\n",
    "        gc.collect()\n",
    "        print(f\"  Memory cleanup after subject {subject_idx + 1}\")\n",
    "\n",
    "# Check if we have any data before final concatenation\n",
    "if len(all_windows) == 0:\n",
    "    raise ValueError(\"No data was successfully processed! Check dataset path and file permissions.\")\n",
    "\n",
    "print(\"Final concatenation of all processed data...\")\n",
    "\n",
    "# Concatenate with error checking\n",
    "try:\n",
    "    final_windows = np.concatenate(all_windows, axis=0)\n",
    "    final_labels = np.concatenate(all_labels, axis=0)\n",
    "    final_subjects = np.array(all_subjects)\n",
    "    \n",
    "    print(f\"Windows shape: {final_windows.shape}\")\n",
    "    print(f\"Labels shape: {final_labels.shape}\")\n",
    "    print(f\"Subjects: {len(final_subjects)} total\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during concatenation: {e}\")\n",
    "    raise ValueError(\"Failed to concatenate data\")\n",
    "\n",
    "# Handle spectral features concatenation\n",
    "final_spectral_features = []\n",
    "for scale_idx in range(len(config['features']['window_sizes'])):\n",
    "    if len(all_spectral_features[scale_idx]) > 0:\n",
    "        try:\n",
    "            concatenated = np.concatenate(all_spectral_features[scale_idx], axis=0)\n",
    "            final_spectral_features.append(concatenated)\n",
    "            print(f\"Scale {scale_idx} spectral features shape: {concatenated.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating spectral features for scale {scale_idx}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"No spectral features found for scale {scale_idx}\")\n",
    "\n",
    "print(f\"\\nFinal data processing summary:\")\n",
    "print(f\"Total windows: {len(final_windows)}\")\n",
    "print(f\"Total data size: {final_windows.nbytes / 1024**3:.2f} GB\")\n",
    "print(f\"Class distribution: {pd.Series(final_labels).value_counts().to_dict()}\")\n",
    "print(f\"Unique subjects: {len(np.unique(final_subjects))}\")\n",
    "print(f\"Spectral features shapes: {[f.shape for f in final_spectral_features]}\")\n",
    "print(f\"Total spectral features size: {sum(f.nbytes for f in final_spectral_features) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Final cleanup\n",
    "del all_windows, all_labels, all_spectral_features\n",
    "gc.collect()\n",
    "print(\"Full dataset processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776413cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the spectral decomposition module\n",
    "import importlib\n",
    "import src.spectral_decomposition\n",
    "importlib.reload(src.spectral_decomposition)\n",
    "from src.spectral_decomposition import MultiScaleSTFT\n",
    "\n",
    "# Recreate the STFT extractor with the fixed code\n",
    "stft_extractor = MultiScaleSTFT(\n",
    "    fs=config['data']['target_fs'],\n",
    "    window_sizes=config['features']['window_sizes'],\n",
    "    hop_ratio=config['features']['hop_ratio']\n",
    ")\n",
    "\n",
    "print(\"Module reloaded and extractor recreated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a803480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables reset for clean processing run.\n"
     ]
    }
   ],
   "source": [
    "# Reset variables for clean data processing\n",
    "all_windows, all_labels, all_subjects = [], [], []\n",
    "all_spectral_features = [[] for _ in config['features']['window_sizes']]\n",
    "print(\"Variables reset for clean processing run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f42280",
   "metadata": {},
   "source": [
    "### 6. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29c4d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing graph...\n",
      "Graph constructed and moved to device.\n"
     ]
    }
   ],
   "source": [
    "print(\"Constructing graph...\")\n",
    "graph_info = create_graph_from_windows(\n",
    "    final_windows, \n",
    "    channels, \n",
    "    method=config['graph']['method']\n",
    ")\n",
    "# Move graph tensors to the correct device\n",
    "for key in ['adjacency', 'laplacian', 'eigenvalues', 'eigenvectors']:\n",
    "    graph_info[key] = graph_info[key].to(device)\n",
    "print(\"Graph constructed and moved to device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb2251",
   "metadata": {},
   "source": [
    "### 7. Update Config and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51db83d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated class weights: [1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "config['model']['n_channels'] = final_windows.shape[1]\n",
    "config['model']['spectral_features_dims'] = [f.shape[1] for f in final_spectral_features]\n",
    "\n",
    "# Update class weights based on data imbalance\n",
    "class_counts = pd.Series(final_labels).value_counts()\n",
    "if 1 in class_counts and 0 in class_counts:\n",
    "    weight_for_class_0 = len(final_labels) / (2 * class_counts[0])\n",
    "    weight_for_class_1 = len(final_labels) / (2 * class_counts[1])\n",
    "    config['trainer']['class_weights'] = [weight_for_class_0, weight_for_class_1]\n",
    "else:\n",
    "    # Handle case where one class is missing in the processed subset\n",
    "    config['trainer']['class_weights'] = [1.0, 1.0]\n",
    "\n",
    "print(f\"Calculated class weights: {config['trainer']['class_weights']}\")\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = EEGDataset(\n",
    "    windows=final_windows,\n",
    "    labels=final_labels,\n",
    "    spectral_features=final_spectral_features,\n",
    "    subjects=final_subjects,\n",
    "    training=False # Augmentation is handled inside the trainer/validator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741187d",
   "metadata": {},
   "source": [
    "### 8. Run Leave-One-Subject-Out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3843dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Leave-One-Subject-Out cross-validation...\n",
      "\n",
      "----- Fold 1/3 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Leave-One-Subject-Out cross-validation...\n",
      "\n",
      "----- Fold 1/3 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Leave-One-Subject-Out cross-validation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m validator = LeaveOneSubjectOutValidator(\n\u001b[32m      9\u001b[39m     model_config=config[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     trainer_config=config[\u001b[33m'\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m results = \u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_folds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GraphFAN/src/training.py:439\u001b[39m, in \u001b[36mLeaveOneSubjectOutValidator.validate\u001b[39m\u001b[34m(self, dataset, graph_info, n_folds)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    438\u001b[39m save_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcheckpoints/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m best_model_path = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m    442\u001b[39m model.load_state_dict(torch.load(best_model_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GraphFAN/src/training.py:279\u001b[39m, in \u001b[36mGFANTrainer.train\u001b[39m\u001b[34m(self, train_loader, val_loader, epochs, save_dir)\u001b[39m\n\u001b[32m    276\u001b[39m patience_counter = \u001b[32m0\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     train_loss, train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     val_loss, val_metrics = \u001b[38;5;28mself\u001b[39m.validate_epoch(val_loader)\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_losses.append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GraphFAN/src/training.py:163\u001b[39m, in \u001b[36mGFANTrainer.train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    159\u001b[39m progress_bar = tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     spectral_features = [\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m'\u001b[39m\u001b[33mspectral_features\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m    164\u001b[39m     labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "print(\"Starting Leave-One-Subject-Out cross-validation...\")\n",
    "validator = LeaveOneSubjectOutValidator(\n",
    "    model_config=config['model'],\n",
    "    trainer_config=config['trainer']\n",
    ")\n",
    "\n",
    "results = validator.validate(\n",
    "    dataset=full_dataset,\n",
    "    graph_info=graph_info,\n",
    "    n_folds=config['validation']['n_folds']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262eccaa",
   "metadata": {},
   "source": [
    "### 9. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efc3d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 2/3: chb02\n",
      "  Processing file 1/10: chb02_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 2/3: chb02\n",
      "  Processing file 1/10: chb02_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  67%|██████▋   | 2/3 [03:42<01:51, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 2/3: chb02\n",
      "  Processing file 1/10: chb02_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  67%|██████▋   | 2/3 [03:42<01:51, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 2: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 3/3: chb03\n",
      "  Processing file 1/10: chb03_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 2/3: chb02\n",
      "  Processing file 1/10: chb02_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  67%|██████▋   | 2/3 [03:42<01:51, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 2: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 3/3: chb03\n",
      "  Processing file 1/10: chb03_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|██████████| 3/3 [05:33<00:00, 111.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient data processing...\n",
      "Using temporary directory: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "Processing 3 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subject 1/3: chb01\n",
      "  Processing file 1/10: chb01_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb01_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb01_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb01_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb01_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb01_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb01_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb01_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb01_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb01_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  33%|███▎      | 1/3 [01:48<03:36, 108.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 2/3: chb02\n",
      "  Processing file 1/10: chb02_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb02_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb02_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb02_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb02_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb02_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb02_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb02_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb02_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb02_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects:  67%|██████▋   | 2/3 [03:42<01:51, 111.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 2: 5000 total windows saved to temp file\n",
      "\n",
      "Processing subject 3/3: chb03\n",
      "  Processing file 1/10: chb03_01.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 2/10: chb03_02.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 3/10: chb03_03.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 4/10: chb03_04.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 5/10: chb03_05.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 6/10: chb03_06.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 7/10: chb03_07.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 8/10: chb03_08.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 9/10: chb03_09.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Successfully processed 500 windows\n",
      "  Processing file 10/10: chb03_10.edf\n",
      "    Extracted 500 windows\n",
      "    Extracted 500 windows\n",
      "    Successfully processed 500 windows\n",
      "    Successfully processed 500 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|██████████| 3/3 [05:33<00:00, 111.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subject chb-mit-scalp-eeg-database-1.0.0/chb03: [Errno 28] No space left on device\n",
      "\n",
      "Data processing summary:\n",
      "- Total files processed: 30\n",
      "- Total windows processed: 15000\n",
      "- Subjects with data: 2\n",
      "- Temporary files created in: /var/folders/q1/0h_x7yc145l73wy2vgm5fd300000gn/T/tmpywq4u_vw\n",
      "SUCCESS: Data processing completed. Temporary files created for memory efficiency.\n",
      "Next step: Load and combine data as needed for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation finished. Saving results...\")\n",
    "os.makedirs(os.path.dirname(config['results_path']), exist_ok=True)\n",
    "validator.save_results(config['results_path'])\n",
    "\n",
    "print(f\"Results saved to {config['results_path']}\")\n",
    "print(\"\\n--- Summary Metrics ---\")\n",
    "summary = validator.get_summary_metrics()\n",
    "if summary:\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbece96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A6000 GPU Optimization Settings\n",
    "print(\"=== A6000 GPU Optimization Settings ===\")\n",
    "\n",
    "# Enable optimizations for maximum performance\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Set memory management\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"Max GPU memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Display final memory usage\n",
    "total_params = sum(p.numel() for p in validator.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in validator.model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024**2:.1f} MB (FP32)\")\n",
    "\n",
    "# Show dataset statistics\n",
    "print(f\"\\n=== Dataset Statistics ===\")\n",
    "print(f\"Total samples processed: {len(final_windows):,}\")\n",
    "print(f\"Memory usage - Windows: {final_windows.nbytes / 1024**3:.2f} GB\")\n",
    "print(f\"Memory usage - Features: {sum(f.nbytes for f in final_spectral_features) / 1024**3:.2f} GB\")\n",
    "print(f\"Total dataset memory: {(final_windows.nbytes + sum(f.nbytes for f in final_spectral_features)) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Estimated training time\n",
    "estimated_time_per_epoch = len(final_windows) / (config['trainer']['batch_size'] * 60)  # minutes\n",
    "print(f\"Estimated time per epoch: {estimated_time_per_epoch:.1f} minutes\")\n",
    "print(f\"Total estimated training time: {estimated_time_per_epoch * config['trainer']['epochs'] / 60:.1f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
